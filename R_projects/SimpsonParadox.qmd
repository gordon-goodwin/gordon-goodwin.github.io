---
title: "Simpson's Paradox"
author: "Gordon Goodwin"
format: html
code-fold: true
code-link: true
code-line-numbers: true
theme: cerulean
toc: true
toc-depth: 4
toc-title: "Contents"
number-sections: false
editor: visual
self-contained: true
whitespace: small
link-external-icon: true
execute: 
  warning: false
  error: false
  cache: false
---

# Overview

**Simpson's Paradox** describes a phenomenon in which a statistical relationship observed within a group or population is **reversed** when examined within *subgroups* of the data.

Or, equivalently, an association first observed within data subgroups can reverse when the groups are *combined*.

**Simpson's paradox is particularly common in HR and employee data**.

In this guide we'll use 2 people analytics case studies to illustrate how to identify and control for the paradox using `R`.

::: callout-caution
## Note: Both examples are inspired by real-life case studies I've encountered during my work in people analytics, but the data we'll be using is simulated for privacy/IP considerations.
:::

------------------------------------------------------------------------

# Case Study #1

In our first example, imagine we've been asked to **examine the** **relationship between salary and seniority** (time on the job) for employees of a small but growing tech company.

**Here's some quick context:**

-   The company's Chief People Officer (CPO) is a staunch advocate of rewarding seniority with increased compensation opportunities.

-   The CPO launched a new compensation structure about a year ago that was designed to reflect this.

[**Has the new structure been effective? Is there a stronger association between salary and seniority?**]{style="color:green; font-weight=bold"}

------------------------------------------------------------------------

### Data & Set-Up

```{r}
#| label: packages

# Load Packages
library(tidyverse)
library(janitor)
library(scales)
library(ggfun)
library(ggtext)
library(ggmosaic)
library(patchwork)
library(broom)
library(GGally)

# Colors from MetBrewer package's Java Palette
metcols <- MetBrewer::met.brewer("Java")

```

We'll begin by simulating a simple dataset containing **salary**, **seniority**, and **department** for 1000 employees.

We can preview the data below:

```{r}
#| label: datasimulation & setup
#| warning: false
#| message: false


# Paradox #1
## Salary vs Seniority is NEGATIVELY correlated overall
### Salary is POSITIVELY correlated with Seniority WITHIN Departments
### Confound is due to the higher-paid departments containing newer hires

deps <- c("DataScience","HR","Finance")

set.seed(12345)
dat2 <- data.frame(department = rep(deps, times = c(333, 333, 333)))
 
# Split by Department - we'll set up the trends within each department
split(dat2, dat2$department) -> dep_groups

#####################################
# Correlations WITHIN groups
######################################

# Data Science 
###  positive cor(salary, seniority)
### HIGH avg salary and LOW avg Seniority

## DS Salary = Mean of 200k, SD of 30k
## DS Tenure weeks = Mean of 120w, SD of 30w
## Cor(DS Salary, DS Tenure) = 0.3

n_DS <- nrow(dep_groups$DataScience)

# Mean vector = Avgs for seniority weeks and salary (k)
mean_vec_DS <- c(120, 200)

# create the variance covariance matrix
## diags = var of X and Y respectively = 30^2 and 30^2 = 900 and 900
## COV = Cor * SX * SY -> 0.3 * 30 * 30 = 270
covmat_DS <- matrix(data = c(900, 270,
                             270, 900),
                    nrow = 2,
                    byrow = T)

# generate the multivariate normal distribution and store as dataframe
dat2_DS <-as.data.frame(MASS::mvrnorm(n=n_DS, mu=mean_vec_DS, Sigma=covmat_DS))


# Set names back to x and y (defaults to V1, V2, etc...)
names(dat2_DS) <- c("seniority_weeks","salary")

## Department ID
dat2_DS$department <- "DataScience"

####################################################

# Finance 

# Finance
###  positive cor(salary, seniority)
### MEDIUM avg salary and MEDIUM avg Seniority

## Finance Salary = Mean of 160k, SD of 25k
## Finance Tenure weeks = Mean of 180d, SD of 30d
## Cor(Finance Salary, Finance Tenure) = 0.3

# Sample Size
n_Finance <- nrow(dep_groups$Finance)

# Mean vector = Avg for tenure weeks and salary (k)
mean_vec_Finance <- c(180, 160)

# create the variance covariance matrix
## diags = var of X and Y respectively = 30^2 and 25^2 = 900 and 625
## COV = Cor * SX * SY -> 0.3 * 30 * 25 = 225
covmat_Finance <- matrix(data = c(900, 225,
                                  225, 625),
                         nrow = 2,
                         byrow = T)

# generate the multivariate normal distribution and store as dataframe
dat2_Finance <-as.data.frame(MASS::mvrnorm(n=n_Finance, mu=mean_vec_Finance, Sigma=covmat_Finance))

# Set names back to x and y (defaults to V1, V2, etc...)
names(dat2_Finance) <- c("seniority_weeks","salary")

## Department ID
dat2_Finance$department <- "Finance"

##########################################################

# HR
###  positive cor(salary, seniority)
### LOW avg salary and HIGH avg Seniority

## HR Salary = Mean of 120k, SD of 20k
## HR Tenure weeks = Mean of 240d, SD of 30d
## Cor(HR Salary, HR Tenure) = 0.3
n_HR <- nrow(dep_groups$HR)

# Mean vector = MU for tenure weeks and salary (k)
mean_vec_HR <- c(240, 120)

# VARCOV Matrix
## diags = var of X and Y respectively = 30^2 and 20^2 = 900 and 400
## COV = Cor * SX * SY -> 0.3 * 30 * 20 = 180
covmat_HR <- matrix(data = c(900, 180,
                             180, 400),
                    nrow = 2,
                    byrow = T)

# generate the multivariate normal distribution and store as dataframe
dat2_HR <-as.data.frame(MASS::mvrnorm(n=n_HR, mu=mean_vec_HR, Sigma=covmat_HR))

# Set names back to x and y (defaults to V1, V2, etc...)
names(dat2_HR) <- c("seniority_weeks","salary")

## Department ID
dat2_HR$department <- "HR"

#############################################################

# UNION BACK TO COMBINED DATAFRAME
# Marge Back
dat2_DS |> 
  union_all(dat2_Finance) |> 
  union_all(dat2_HR) -> dat2main

# Create Seniority Years instead of weeks (more intuitive)
# Create a salary measured in $k and measured in reg USD
dat2main |> 
  mutate(seniority_years = seniority_weeks/52,
         .after = seniority_weeks) |> 
  mutate(salary_k = salary, 
         salary = salary*1000)-> dat2main

# Round to 2 digits for ease
dat2main |> 
  mutate(across(where(is.numeric),
         ~round(.x, 2))) |> 
  select(department, seniority_weeks, seniority_years,
         salary, salary_k)-> dat2main

## Preview
dat2main |> 
  select(department, seniority_years, salary) |> 
  head()

```

------------------------------------------------------------------------

### Initial Trend

We start our review by taking a quick look at the overall correlation between salary and seniority.

When we do so, the data shows a surprising trend - [it appears that salary and seniority are negatively related!]{style="color:red; font-weight:italic"}

Ugh - the CPO is *not* going to be happy!

```{r}
#| label: salary vs seniority - overall
#| message: false
#| warning: false

dat2main |> 
  ggplot(aes(x = seniority_years,
             y = salary)) +
  geom_point(size = 1, alpha = 0.7) +
  geom_smooth(method = "lm", se = F) +
  scale_y_continuous(labels = label_dollar()) +
  labs(x = "Seniority Years", y = "Salary") +
  labs(x = "Seniority Years", y = "Salary",
       title = "Salary Appears Negatively Related to Seniority") +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "italic", color = "#cf3a36")) -> p1

# Display
p1

```

We decide to run a regression model predicting salary from seniority weeks.

When we do so, the model output below confirms the trend and shows that **seniority is a significant *negative* predictor of salary**, with a slope estimate of *`b = -$22,204`*.

::: callout-important
## Holding all other factors constant, an employee with 1-year more seniority than their peers is expected to have a salary that is over \$22,000 lower!
:::

```{r}
#| label: sig test - overall correlation

# Linear Regression Model
m1 <- lm(salary ~ seniority_years, data = dat2main)

# Tidy Model Output instead of ugly base R summary
tidy(m1)

```

------------------------------------------------------------------------

### Paradox Uncovered

Just as we start to get worried about how to explain the results to the CPO, we remember one of our old stats professors talking about the need to control for **confounding factors**.

This gets us thinking - we know that:

-   **Several departments are newer** and have teams with much lower seniority levels.

-   **Some departments command much higher average salaries than others** due to our market-based comp strategy

This makes us wonder:

::: callout-caution
## \*Is it possible that department is a confounding factor for the relationship between salary and seniority?\*
:::

**We decide to take a look at the salary-seniority relationship *within departments*.**

```{r}
#| label: trend within departments
#| message: false
#| warning: false

# Plot - WITHIN GROUPS
dat2main |> 
  ggplot(aes(x = seniority_years,
             y = salary,
             color = department)) +
  geom_point(size = 1, alpha = 0.7) +
  geom_smooth(method = "lm", se = F) +
  scale_y_continuous(labels = label_dollar()) +
  scale_color_manual(values = c("#663171", "#cf3a36", "#ea7428")) +
  theme_bw(base_size = 14) +
  theme(legend.position = "bottom") +
  labs(x = "Seniority Years", y = "Salary", color = "Department",
       title = "Trend Reversal when Controlling for Departments") +
  theme(plot.title = element_text(face = "italic", color = "blue")) -> p2

# Display
p2
```

Whoa! Now our initial negative trend has reversed!

**Salary is *positively* related to seniority within all 3 department subgroups**

When we add department as a predictor to our regression model predicting salary from seniority, the output below confirms what we see in our scatterplot.

**After controlling for department, *salary is positively related to seniority***, with a slope of *`b = $13,425`.*

::: callout-important
## Holding all other factors constant, for employees in the same department, each additional year of seniority is associated with an expected salary that is \$13,425 higher!
:::

```{r}
#| label: mult reg test

# Linear Regression Model
m2 <- lm(salary ~ seniority_years + department, data = dat2main)

# Tidy Model Output rounded to 3 digits
tidy(m2) |> 
  mutate(across(where(is.numeric),
                ~round(.x, 3)))

```

::: callout-note
## Data Science serves as the "reference" group in our model, so the negative slopes for HR and Finance indicate lower average salaries relative to the DS team.
:::

Let's take one last side-by-side look:

![](cs1_sidebyside.png)

------------------------------------------------------------------------

### Explanation

So how did this happen?

[How can salary be positively related to seniority within all 3 departments*, but not overall?*]{style="color:purple; font-weight:bold"}

The paradox arises because **average salary and seniority levels both vary significantly between departments**.

The plot below helps visualize how the 3 variables are intertwined:

-   ***Within*** **each department, greater seniority is associated with greater compensation**

-   **Data Science is a less-senior but highly compensated team** comprised of newer hires who command higher salaries than their peers in other departments

-   **HR employees have high levels of seniority but lower levels of compensation** relative to the rest of the company

-   **Finance employees have medium levels of compensation and seniority** relative to the HR and Data Science departments

```{r}
#| label: ggpairs
#| message: false
#| warning: false

GGally::ggpairs(dat2main |> select(-c(salary_k,seniority_weeks)), aes(color = department, alpha = 0.4)) + 
  scale_color_manual(values = metcols) +
  scale_fill_manual(values = metcols)

```

::: callout-important
## In summary, while greater seniority is associated with lower salary when the employees are viewed collectively, that trend reverses after controlling for disparities between departments.
:::

------------------------------------------------------------------------

# Case Study #2

In our second example, imagine the same Chief People Officer has now asked us to take a look at the results of a recent survey asking employees whether they view their job satisfaction favorably or unfavorably.

Specifically, the CPO wants us to **examine the relationship between employee sourcing and job satisfaction favorability.**

**Here's some quick context:**

-   About 6 months ago the Talent Acquisition team began offering a big referral bonus to incumbent employees who refer a hired candidate that retains 90 weeks or more

-   Bonus was launched in part because the *CPO hoped that new hires sourced through referrals would have higher levels of engagement, satisfaction, and retention.*

**The CPO now wants us to determine whether referred employees are more likely to rate their job satisfaction favorably than employees who applied traditionally.**

------------------------------------------------------------------------

### Data & Set-Up

We'll begin by simulating a simple dataset containing referral status, work location, and **job statisfaction** **favorability** for 1000 employees.

We can preview the data below:

```{r}
#| label: paradox2 data simulation
#| message: false
#| warning: false


# Paradox 2: Satisfaction & referral Confounded by location 
## Appears at first that referrals are LESS satisfied than non-referrals
## BUT when partitioned by work location, referrals are MORE satisfied


# Sample Size for referrals/non-referrals
n_referrals <- 300
n_nonref <- 700

referral <- c("Referred","Applied")
location <- c("Remote","Office")
satisfied <- c("Favorable","Unfavorable")

# Initialize Dataframe w/ 100 men, 100 women
dat1 <- data.frame(referral = rep(referral, times = c(n_referrals, n_nonref)))

set.seed(12345)

# Location Disparity amongst referral/non-referral
## NON-Referrals much more likely to be Remote employees
dat1 |> 
  rowwise() |> 
  mutate(location = case_when(referral == "Applied" ~ sample(location, 1, prob = c(.7, .3)),
                              referral == "Referred" ~ sample(location, 1, prob = c(.2, .8)))) |> 
  ungroup() -> dat1


# Split into Remote  vs Not Remote
split(dat1, dat1$location) -> location_groups



# Set Satisfaction rates by LOCATION GROUP
## Goal is to have:
### Referrals  SLIGHTLY HIGHER in Satisfaction WITHIN both Location groups
### Remote satisfaction overall MUCH HIGHER satisfaction
### Paradox will result due to majority of referrals being In-Office
#### due to unequal location balance by referral/non-referral status

# REMOTE
## High overall Satisfaction with Referrals slightly higher than Non-Ref
location_groups$Remote |> 
  rowwise() |> 
  mutate(satisfied = case_when(referral == "Applied" ~ sample(satisfied,1, prob = c(.7,.3)),
                               referral == "Referred" ~ sample(satisfied, 1, prob = c(.85, .15)))) |> 
  ungroup() -> dat1_Remote

# OFFICE
## Lower overall satisfaction w/ Referrals slightly higher than Non-Ref
location_groups$Office |> 
  rowwise() |> 
  mutate(satisfied = case_when(referral == "Applied" ~ sample(satisfied,1, prob = c(.15,.85)),
                               referral == "Referred" ~ sample(satisfied, 1, prob = c(.35, .65)))) |> 
  ungroup() -> dat1_Office

# Merge Back to Main Dataframe
dat1_Remote |> 
  union_all(dat1_Office) -> dat1combined

# Add Employee ID (Random, not used for anything)
dat1combined |> 
  mutate(emp_ID = paste0("x",1:nrow(dat1combined)),
         .before = referral) -> dat1combined 

# Glimpse
dat1combined |> head()

```

------------------------------------------------------------------------

### Initial Trend

We start our review by taking a quick look at the overall relationship between referral status (Referred vs Applied) and job satisfaction (Favorable vs Unfavorable).

Once again our initial review is surprising - it suggests that [*employees sourced via referral are actually **less likely to be satisfied** than employees who applied traditionally!*]{style="color:red"}

Dang it! How are going to explain this to the CPO?

```{r}
#| label: initial trend - referral vs satisfaction
#| message: false
#| warning: false

dat1combined |> 
  count(referral, satisfied) |> 
  mutate(prop = round(n/sum(n),2),
         prop_label = glue::glue("{scales::percent(prop)} ({n})"),
         .by = referral) |>
  mutate(satisfied = fct_relevel(satisfied, c("Unfavorable","Favorable"))) |> 
  # filter(satisfied == "Favorable") |> 
  ggplot(aes(x = referral,
             y = prop,
             fill = satisfied)) +
  geom_col(width = 0.4) +
  scale_y_continuous(labels = scales::label_percent()) +
  coord_cartesian(ylim = c(0, 1)) +
  geom_text(aes(label = prop_label),
            position = position_stack(vjust = .4),
            colour = "white", size = 4,
            fontface = "bold") +
  labs(x = "Pre-Hire Source",
       y = "Responded Favorably",
       fill = "Job Satisf.",
       title = "Referrals are *Less* Likely to be Satisfied") +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "italic", color = "#cf3a36")) +
  scale_fill_manual(values = c("#cf3a36","green4"))


```

We decide to run a Chi-Squared test to examine whether job satisfaction is statistically independent of referral status.

When we do so, the test output below confirms that **job satisfaction is *not* statistically independent of referral status**

::: callout-important
## Whether an employee views their job satisfaction favorably is dependent upon whether or not they were sourced via referral.
:::

```{r}
#| label: chisq test

# Chi Sq Test
m1b <- chisq.test(dat1combined$referral, dat1combined$satisfied)

# model output 
print(m1b)

```

------------------------------------------------------------------------

### Paradox Uncovered

While this obviously isn't the result our CPO was hoping for, we recall the lessons learned from our first example regarding **confounding factors**.

We know that:

-   Our company is currently a **mix of remote-first and hybrid/office-based** employees

-   Past surveys have shown that our **In-Office employees** **tend to have** **much lower satisfaction** levels than their Remote peers.

This makes us wonder:

::: callout-caution
## \*Is it possible that work location is a confounding factor?\*
:::

**We decide to take a look at the referral-satisfaction relationship *within work locations*.**

```{r}
#| label: paradox2 uncovered

dat1combined |> 
  count(referral, satisfied, location) |> 
  mutate(prop = round(n/sum(n),2),
         prop_label = glue::glue("{scales::percent(prop)} ({n})"),
         .by = c(referral, location)) |>
    mutate(satisfied = fct_relevel(satisfied, c("Unfavorable","Favorable"))) |> 
 # filter(satisfied == "Favorable") |> 
  ggplot(aes(x = referral,
             y = prop,
             fill = satisfied)) +
  geom_col(width = 0.8, color = "black") +
  scale_y_continuous(labels = scales::label_percent()) +
  coord_cartesian(ylim = c(0, 1)) +
  geom_text(aes(label = prop_label),
            position = position_stack(vjust = 0.5),
            colour = "white", size = 4,
            fontface = "bold") +
  facet_wrap(~location) +
  labs(x = "Pre-Hire Source",
       y = "Responded Favorably",
       fill = "Job Satisf.",
       title = "Referrals are *More* Satisfied within each Location") +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "italic", color = "blue"),
        strip.background = element_roundrect(fill = "wheat",color = "black", r = 0.15),
        strip.text.x = element_text(color = "black", face = "bold"),
        axis.text = element_text(color = "black"),
        axis.title = element_text(color = "black"),
        legend.position = "top") +
  scale_fill_manual(values = c("#cf3a36","green4"))

```

Deja-vu! Our initial trend has reversed!

[Referred employees are *more* likely to be satisfied than traditionally-sourced employees *within both work location subgroups*]{style="color:green; font-weight:bold"}

When we use a logistic regression model to predict satisfaction ratings from referral status and work location, the results are similar:

**After controlling for work location, *referral status is a positive predictor of job satisfaction favorability***, with a slope of *`exp(b) = 2.5`*

::: callout-important
## Holding all else constant, referred employees are 2.5x more likely to rate their satisfaction favorably.
:::

```{r}
#| label: logistic regression model confirming results
#| message: false

# Convert Satisfaction (Favorable/Unfavorable) to 1/0 for model
dat1combined$satisfied_recoded <- ifelse(dat1combined$satisfied == "Favorable",1,0)

# Relabel referral and location for legibility
dat1combined$referral <- factor(dat1combined$referral, labels = c("No","Yes"))

# Logistic regression model
m3 <- glm(satisfied_recoded ~ referral + location, data = dat1combined, family = "binomial")

# Tidy Model Output rounded to 3 digits
tidy(m3) |> 
  mutate(across(where(is.numeric),
                ~round(.x, 3))) |> 
  # Exponentiate to get odds ratio 
  mutate(`exp(estimate)` = exp(estimate), .before = estimate)
```

::: callout-note
## Non-referred employees who work in-office are the "reference group" for our model, and the Remote slope indicates that remote employees are 12.7x more likely to be satisfied than in-office employees.
:::

------------------------------------------------------------------------

### Explanation

Again we ask, so why did this happen?

[How can referred employees be more satisfied within both work locations *but not overall?*]{style="color:purple; font-weight:bold"}

[*The paradox arises because referral status and job satisfaction are **imbalanced across work locations*****.**]{style="color:red"}

As seen in the plot below:

-   **Referred employees are more likely to be In-Office**

-   **In-Office employees are less likely to have favorable job satisfaction**

-   ***Within*** ***each work location*****, referred employees are more likely to be satisfied**

```{r}
#| label: bubbleplot viz

# Crosstab
dat1combined |> 
  summarize(prop_satisfied = mean(satisfied == "Favorable"),
            count_satisfied = sum(satisfied == "Favorable"),
            count_NOTsatisfied = sum(satisfied != "Favorable"),
            n = n(),
            .by = c(referral, location)) |> 
  mutate(prop_label = glue::glue("(n={n}) Satisf = {scales::percent(prop_satisfied)}")) |> 
  arrange(referral, location) -> tab1

# Plot
tab1 |> 
  mutate(referral = if_else(referral=="Yes","Referred","Applied")) |> 
  ggplot(aes(x = referral, y = location)) +
  geom_point(aes(size = n, color = prop_satisfied)) +
  scale_color_gradient2(low = "#cf3a36", 
                        mid = "grey70", 
                        high = "#0c7156",
                        midpoint = .5,
                        labels = scales::label_percent()) +
  # scale_color_viridis_c(option = "H", direction = 1) +
  scale_size_area(max_size = 10) +
  theme_bw(base_size = 14) +
  theme(legend.position = "right",
        axis.text = element_text(size = 14, color = "black"),
        axis.title = element_text(size = 16, color = "blue"),
        plot.title = element_text(size = 16, color = "#663171", face = "italic"),
        legend.background = element_rect(color = "black")) +
  labs(x = "Source", y = "Location", 
       size = "Emps (#)", color = "Satisf.",
       title = "Work Location is a Confound for Satisfaction Rates") +
  geom_text(aes(x = referral, 
                y = location, 
                label = prop_label),
            color = "blue",
            vjust = -2)


```

We can also visualize the imbalance between work location and referral status with a Mosaic plot:

```{r}
#| label: mosaic viz of disparity
#| message: false
#| warning: false

# Mosaic Plot of Sample Size
dat1combined |> 
  mutate(referral = factor(referral, labels = c("Applied","Referred")),
         location = fct(location),
         satisfied = fct(satisfied)) |> 
  ggplot() +
  geom_mosaic(aes(x = product(location, referral), fill = location), show.legend = T) +
  theme_mosaic() +
  scale_fill_manual(values = c("green4", "#ea7428")) +
  coord_flip() +
  labs(x = "Work Location",
       y = "Pre-Hire Source",
       fill = "Location",
       title = "Work Location Imbalance Across Referral Groups") +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(color = "#663171", face = "italic"),
        legend.position = "top")

```

[In summary, while referred employees are less likely to rate their job satisfaction favorably when viewed *collectively*, that trend *reverses* after controlling for *disparities between work locations*.]{style="color:green; font-weight:bold"}

------------------------------------------------------------------------

# Final Notes

First, if you've made it this far, first you have my sincere appreciation!

Second, this type of paradox is a frequent occurrence in employee/HR data, but controlling for it can be tricky.

**Context is everything, and the best approach will depend heavily upon your exact data, business question, and limitations.**

Typically, the following main approaches are considered (one only applies for experimental data):

-   **Sub-group Analysis:** Identifying possible confounds through exploratory data visualization and then analyzing the trend of interest *within* subgroups

    -   This is the approach we took in this guide!

-   **Weighted Averages:** Identifying possible confounds through data viz and then using *weighted averages* to mitigate sample size group imbalances

-   **Randomized Controlled Trial (RCT)**: This is the gold standard for experimental design, especially when combined with a double-blind procedure.

### Suggested Resources

If you're interested in learning more about Simpson's Paradox, I highly recommend the following resources:

-   <https://www.youtube.com/watch?v=QapqOK2_7z8>

-   <https://www.youtube.com/watch?v=t-Ci3FosqZs>

-   <https://www2.stat.duke.edu/~banks/611-lectures.dir/lect1.pdf>
